# ì—…ë¡œë“œ í›„ ìžë™ ì¡°íšŒ(í´ë§) ê´€ë ¨ ì„¤ì •
POLL_TRIES = 10       # ìž¬ì‹œë„ íšŸìˆ˜ (ê¸°ë³¸ 10íšŒ â‰ˆ ìµœëŒ€ 30ì´ˆ)
POLL_INTERVAL = 3     # ìž¬ì‹œë„ ê°„ê²©(ì´ˆ)

# pages/1_ðŸ“„_ë‚´ë¶€_ë¬¸ì„œ_ë¶„ì„.py
import streamlit as st
import requests, uuid
import config, utils
from ui import H2, H3, _clean_citations

# ====================== ì„¸ì…˜ ê¸°ë³¸ê°’ ======================
if "doc_hits" not in st.session_state:
    st.session_state["doc_hits"] = []
if "last_blob_name" not in st.session_state:
    st.session_state["last_blob_name"] = ""

# ---------------------- í—¬í¼: rerun/ì´ˆê¸°í™” ----------------------
def _rerun():
    if hasattr(st, "rerun"):
        st.rerun()
    elif hasattr(st, "experimental_rerun"):
        st.experimental_rerun()

def _clear_analysis_state():
    for k in ("doc_hits", "last_blob_name"):
        st.session_state.pop(k, None)
    for k in ("news_results", "pest_swot_json", "combined_json", "pdf_sig"):
        st.session_state.pop(k, None)
    st.cache_data.clear()
    st.cache_resource.clear()

# -------------------------------------------------------------------
# ìœ í‹¸: ì¸ë±ì„œ ìƒíƒœ ì¡°íšŒ (ê³ ê¸‰ ê¸°ëŠ¥)
# -------------------------------------------------------------------
def _get_indexer_status():
    try:
        url = f"{config.SEARCH_ENDPOINT}/indexers/{config.SEARCH_INDEXER}/status?api-version={config.SEARCH_API_VER}"
        r = requests.get(url, headers={"api-key": config.SEARCH_KEY}, timeout=20)
        r.raise_for_status()
        return r.json() or {}
    except Exception as e:
        st.error(f"ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return {}

# -------------------------------------------------------------------
# í—¤ë”

# -------------------------------------------------------------------
# 1) PDF ì—…ë¡œë“œ (Blobë§Œ)
# -------------------------------------------------------------------
st.subheader("ðŸ“„ ë‚´ë¶€ ë¬¸ì„œ ì—…ë¡œë“œ")
st.caption("ë‚´ê°€ ì—…ë¡œë“œí•œ PDF íŒŒì¼ì˜ ë‚´ìš©ì„ AIê°€ ê²€ìƒ‰í•´ ë³´ì—¬ì¤ë‹ˆë‹¤.")

upload_file = st.file_uploader("PDF íŒŒì¼ ì„ íƒ", type=["pdf"], key="pdf_upload_for_blob")

col_up = st.columns([1, 1, 2])
with col_up[0]:
    # ðŸ”» ìš´ì˜ UXì—ì„  ìˆ¨ê¸°ëŠ” ê²Œ ê¹”ë” â€” ê³ ê¸‰ Expanderë¡œ ì´ë™
    pass

with col_up[1]:
    auto_fetch = st.checkbox("ì—…ë¡œë“œ í›„ ìžë™ ì¡°íšŒ", value=True, help="ì—…ë¡œë“œê°€ ì„±ê³µí•˜ë©´ í•´ë‹¹ íŒŒì¼ ê¸°ì¤€ìœ¼ë¡œ ë°”ë¡œ ì¸ë±ìŠ¤ ê²€ìƒ‰")

with col_up[2]:
    last = st.session_state.get("last_blob_name")
    st.info(f"ìµœê·¼ ì—…ë¡œë“œ: `{last}`" if last else "ìµœê·¼ ì—…ë¡œë“œ: (ì—†ìŒ)")

# ì—…ë¡œë“œ ì‹¤í–‰
if st.button("â¬†ï¸ ì—…ë¡œë“œ ì‹¤í–‰", use_container_width=True, key="btn_blob_upload"):
    if not upload_file:
        st.warning("íŒŒì¼ì„ ë¨¼ì € ì„ íƒí•˜ì„¸ìš”.")
    elif not config.blob_container:
        st.error("Blob ì»¨í…Œì´ë„ˆ ì—°ê²° ì‹¤íŒ¨. .envì˜ AZURE_STORAGE_CONN í™•ì¸.")
    else:
        try:
            import time, uuid
            blob_name = f"{uuid.uuid4()}_{upload_file.name}"
            upload_file.seek(0)
            config.blob_container.upload_blob(name=blob_name, data=upload_file, overwrite=True)
            st.session_state["last_blob_name"] = blob_name
            st.success(f"âœ… ì—…ë¡œë“œ ì™„ë£Œ: {blob_name}")

            if auto_fetch:
                # í™”ë©´ì—” ìŠ¤í”¼ë„ˆë§Œ ë³´ì´ê²Œ
                with st.spinner("ðŸ“„ ì¸ë±ì‹± ë°˜ì˜ ì¤‘ìž…ë‹ˆë‹¤... ìž ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”."):
                    hits = []
                    delays = [2, 3, 5, 8, 13, 21]
                    for i, wait in enumerate(delays, start=1):
                        st.cache_data.clear()
                        try:
                            hits = utils.search_docs_by_blobname(st.session_state["last_blob_name"], top=10, nonce=i)
                        except TypeError:
                            hits = utils.search_docs_by_blobname(st.session_state["last_blob_name"], top=10)

                        if hits:
                            break

                        if i in (2, 3) and config.SEARCH_INDEXER:
                            try:
                                run_url = f"{config.SEARCH_ENDPOINT}/indexers/{config.SEARCH_INDEXER}/run?api-version={config.SEARCH_API_VER}"
                                requests.post(run_url, headers={"api-key": config.SEARCH_KEY}, timeout=10)
                            except Exception:
                                pass

                        time.sleep(wait)

                if hits:
                    st.session_state["doc_hits"] = hits
                    st.success(f"âœ… ì¸ë±ì‹± ë°˜ì˜ ì™„ë£Œ! ë¬¸ì„œ ì¡°ê° {len(hits)}ê±´")
                    st.toast(f"{len(hits)}ê±´ ë¡œë“œë¨")
                else:
                    st.session_state["doc_hits"] = []
                    st.warning("âš ï¸ ì•„ì§ ì¸ë±ì‹± ëŒ€ê¸° ì¤‘ìž…ë‹ˆë‹¤. ìž ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")

        except Exception as e:
            st.error(f"ì—…ë¡œë“œ ì˜¤ë¥˜: {e}")

# -------------------------------------------------------------------
# â›” ê°•ì œ ìž¬ì‹œìž‘(Reset â†’ Run) í›„ ë‚´ íŒŒì¼ ìž¬ì¡°íšŒ  + ì„¸ì…˜ ì´ˆê¸°í™”ëŠ” ê³ ê¸‰ìœ¼ë¡œ ì´ë™
# -------------------------------------------------------------------
with st.expander("âš™ï¸ ê³ ê¸‰: ë””ë²„ê·¸/ë³µêµ¬ ë„êµ¬", expanded=False):
    c1, c2 = st.columns(2)

    with c1:
        if st.button("ðŸ§¹ ì„¸ì…˜ ì´ˆê¸°í™” (ì•± ë‹¤ì‹œ ì‹œìž‘)", use_container_width=True, key="btn_reset_pdf_here"):
            _clear_analysis_state()
            _rerun()

    with c2:
        if st.button("â›” ê°•ì œ ìž¬ì‹œìž‘(Resetâ†’Run) í›„ ìž¬ì¡°íšŒ", use_container_width=True, key="btn_force_reset_run"):
            last = st.session_state.get("last_blob_name", "")
            if not last:
                st.warning("ë¨¼ì € PDFë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.")
            elif not (config.SEARCH_ENDPOINT and config.SEARCH_INDEXER and config.SEARCH_KEY and config.SEARCH_API_VER):
                st.error("Azure Search ì„¤ì •(ENDPOINT/INDEXER/KEY/API_VER)ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
            else:
                import time, requests
                try:
                    headers = {"api-key": config.SEARCH_KEY}
                    base = f"{config.SEARCH_ENDPOINT}/indexers/{config.SEARCH_INDEXER}"

                    # Reset â†’ Run
                    r_reset = requests.post(f"{base}/reset?api-version={config.SEARCH_API_VER}", headers=headers, timeout=20)
                    if r_reset.status_code not in (204, 202):
                        st.warning(f"Reset ì‘ë‹µ: {r_reset.status_code} {r_reset.text}")

                    r_run = requests.post(f"{base}/run?api-version={config.SEARCH_API_VER}", headers=headers, timeout=20)
                    if r_run.status_code not in (202, 204):
                        st.warning(f"Run ì‘ë‹µ: {r_run.status_code} {r_run.text}")

                    with st.spinner("ðŸ“„ ì¸ë±ì‹± ë°˜ì˜ ëŒ€ê¸° ì¤‘..."):
                        found = []
                        for i in range(12):   # ìµœëŒ€ ~36ì´ˆ
                            st.cache_data.clear()
                            try:
                                found = utils.search_docs_by_blobname(last, top=10, nonce=i)
                            except TypeError:
                                found = utils.search_docs_by_blobname(last, top=10)
                            if found:
                                break
                            time.sleep(3)

                    if found:
                        st.session_state["doc_hits"] = found
                        st.success(f"âœ… ì¸ë±ì‹± ë°˜ì˜ë¨: ì¡°ê° {len(found)}ê±´")
                        st.toast("ìš”ì•½ ì˜ì—­ì„ ì•„ëž˜ì—ì„œ í™•ì¸í•˜ì„¸ìš”.")
                    else:
                        st.info("ì•„ì§ ë°˜ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ìž ì‹œ í›„ ë‹¤ì‹œ ëˆŒëŸ¬ë³´ì„¸ìš”.")
                except Exception as e:
                    st.error(f"ê°•ì œ ìž¬ì‹œìž‘ ì‹¤íŒ¨: {e}")

    st.markdown("---")
    if st.button("ðŸƒ ì¸ë±ì„œ ìˆ˜ë™ ì‹¤í–‰", use_container_width=True, disabled=not config.SEARCH_INDEXER):
        try:
            run_url = f"{config.SEARCH_ENDPOINT}/indexers/{config.SEARCH_INDEXER}/run?api-version={config.SEARCH_API_VER}"
            r = requests.post(run_url, headers={"api-key": config.SEARCH_KEY}, timeout=20)
            if r.status_code == 202:
                st.success("âœ… ì‹¤í–‰ íŠ¸ë¦¬ê±° ì„±ê³µ")
            elif r.status_code == 409:
                st.warning("âš ï¸ ì´ë¯¸ ì‹¤í–‰ ì¤‘(409). ìž ì‹œ í›„ ìƒíƒœ í™•ì¸í•˜ì„¸ìš”.")
            else:
                st.error(f"âŒ ì‹¤íŒ¨: {r.status_code} {r.text}")
        except Exception as e:
            st.error(f"ì‹¤í–‰ ì˜¤ë¥˜: {e}")

    if st.button("ðŸ”Ž ì¸ë±ì„œ ìƒíƒœ í™•ì¸", use_container_width=True, disabled=not config.SEARCH_INDEXER):
        s = _get_indexer_status()
        top = (s.get("status") or "").lower()
        last = s.get("lastResult") or {}
        st.info(f"service={top} Â· last={ (last.get('status') or '').lower() }")
        with st.expander("ì›ë³¸ ìƒíƒœ JSON ë³´ê¸°"):
            st.json(s)

st.divider()

# -------------------------------------------------------------------
# 3) ì¸ë±ìŠ¤ì—ì„œ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸° (í‚¤ì›Œë“œ)
# -------------------------------------------------------------------
st.subheader("ðŸ“„ ì—…ë¡œë“œí•œ ë¬¸ì„œì—ì„œ ì°¾ê¸°")
st.caption("ë‚´ê°€ ì—…ë¡œë“œí•œ PDF íŒŒì¼ì˜ ë‚´ìš©ì„ AIê°€ ê²€ìƒ‰í•´ ë³´ì—¬ì¤ë‹ˆë‹¤.")
q_doc = st.text_input("í‚¤ì›Œë“œë¡œ ê²€ìƒ‰", value="", placeholder="ì˜ˆ: ë³´ì•ˆ ìš”êµ¬ì‚¬í•­, KPI, ì œì•ˆì„œ ìš”ì•½", key="txt_q_doc")

if st.button("â¬†ï¸ í‚¤ì›Œë“œ ê²€ìƒ‰", use_container_width=True, key="btn_fetch_by_kw"):
    query = (q_doc or "").strip()
    if not query:
        st.warning("ê²€ìƒ‰ì–´ë¥¼ ìž…ë ¥í•˜ì„¸ìš”.")
    else:
        hits = utils.search_docs_by_keyword(query, top=8)
        st.session_state["doc_hits"] = hits
        st.success(f"í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ {len(hits)}ê±´")

st.divider()

# -------------------------------------------------------------------
# 4) ë‚´ë¶€ ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼ (ìš”ì•½ 1ê±´)
# -------------------------------------------------------------------
doc_hits = st.session_state.get("doc_hits") or []
H3("ðŸ“š ë‚´ë¶€ ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼ (í†µí•© ìš”ì•½ 1ê±´)")
if not doc_hits:
    st.caption("ì•„ì§ ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ì—…ë¡œë“œ í›„ ìžë™ ì¡°íšŒ ë˜ëŠ” 'í‚¤ì›Œë“œ ê²€ìƒ‰'ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
else:
    top_titles = [(h.get("title") or "(ì œëª© ì—†ìŒ)") for h in doc_hits[:3] if isinstance(h, dict)]
    if top_titles:
        st.caption("ëŒ€í‘œ: " + " | ".join(top_titles))

    safe_hits = [h for h in doc_hits if isinstance(h, dict)]
    try:
        summary = utils.summarize_docs_combined(safe_hits, max_chars=20000)
        st.write(_clean_citations(str(summary).strip()))
    except Exception as e:
        st.warning(f"ìš”ì•½ ì‹¤íŒ¨ â†’ ì¼ë¶€ë§Œ í‘œì‹œ ({e})")
        merged = "".join([(h.get("content") or "") for h in safe_hits])[:600]
        st.write(_clean_citations(merged + ("â€¦" if len(merged) >= 600 else "")))

    with st.expander("ì¡°ê°ë³„ ì›ë¬¸ ì¼ë¶€ ë³´ê¸°"):
        for i, h in enumerate(safe_hits, 1):
            st.markdown(f"**[{i}] {h.get('title') or '(ì œëª© ì—†ìŒ)'}**")
            if h.get("source"):
                st.caption(h["source"])
            content_text = h.get("content") or ""
            if not isinstance(content_text, str):
                content_text = str(content_text or "")
            st.write(_clean_citations(content_text[:800] + ("â€¦" if len(content_text) > 800 else "")))
            st.markdown("---")
